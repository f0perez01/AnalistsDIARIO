# Getting Started - Data Analysis Microservice

Esta guÃ­a te ayudarÃ¡ a configurar y ejecutar el microservicio de anÃ¡lisis de datos diario con orquestador Saga.

## ğŸ“‹ Tabla de Contenidos

- [Requisitos Previos](#requisitos-previos)
- [ConfiguraciÃ³n del Entorno Local](#configuraciÃ³n-del-entorno-local)
- [ConfiguraciÃ³n de Google Cloud](#configuraciÃ³n-de-google-cloud)
- [EjecuciÃ³n Local](#ejecuciÃ³n-local)
- [Despliegue a Cloud Run](#despliegue-a-cloud-run)
- [ConfiguraciÃ³n del Cron Job](#configuraciÃ³n-del-cron-job)
- [Pruebas](#pruebas)
- [Arquitectura](#arquitectura)
- [SoluciÃ³n de Problemas](#soluciÃ³n-de-problemas)

---

## ğŸ”§ Requisitos Previos

### Software Necesario

- **Python 3.10+** - [Descargar](https://www.python.org/downloads/)
- **Google Cloud SDK** - [Instalar gcloud CLI](https://cloud.google.com/sdk/docs/install)
- **Docker** (opcional, para testing local) - [Descargar](https://www.docker.com/get-started)
- **Git** - [Descargar](https://git-scm.com/downloads)

### Cuenta de Google Cloud

- Proyecto de GCP activo
- FacturaciÃ³n habilitada
- APIs habilitadas:
  - Cloud Run API
  - Cloud Scheduler API
  - Firestore API
  - Secret Manager API
  - Cloud Storage API (opcional)
  - BigQuery API (opcional)

---

## ğŸš€ ConfiguraciÃ³n del Entorno Local

### 1. Clonar el Repositorio

```bash
git clone <repository-url>
cd AnalistsDIARIO
```

### 2. Configurar el Entorno Virtual

**OpciÃ³n A: Usando el script automÃ¡tico**

```bash
./scripts/local-setup.sh
```

**OpciÃ³n B: Manual**

```bash
# Crear entorno virtual
python3 -m venv venv

# Activar entorno virtual
# En macOS/Linux:
source venv/bin/activate

# En Windows:
# venv\Scripts\activate

# Instalar dependencias
pip install --upgrade pip
pip install -r requirements.txt
```

### 3. Configurar Variables de Entorno

```bash
# Copiar archivo de ejemplo
cp .env.example .env

# Editar .env con tu configuraciÃ³n
nano .env  # o usa tu editor preferido
```

**ConfiguraciÃ³n mÃ­nima requerida en `.env`:**

```env
# GCP Configuration
GCP_PROJECT_ID=tu-proyecto-id
GOOGLE_APPLICATION_CREDENTIALS=/ruta/a/service-account-key.json

# Workflow Configuration
WORKFLOW_NAME=daily_data_analysis
ENVIRONMENT=development

# Server Configuration
PORT=8080
```

---

## â˜ï¸ ConfiguraciÃ³n de Google Cloud

### 1. AutenticaciÃ³n

```bash
# Iniciar sesiÃ³n en GCP
gcloud auth login

# Configurar proyecto por defecto
gcloud config set project TU_PROJECT_ID

# Autenticar Docker (si usarÃ¡s Container Registry)
gcloud auth configure-docker
```

### 2. Crear Service Account

```bash
# Crear service account
gcloud iam service-accounts create data-analysis-service \
    --display-name="Data Analysis Service" \
    --project=TU_PROJECT_ID

# Asignar roles necesarios
gcloud projects add-iam-policy-binding TU_PROJECT_ID \
    --member="serviceAccount:data-analysis-service@TU_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/datastore.user"

gcloud projects add-iam-policy-binding TU_PROJECT_ID \
    --member="serviceAccount:data-analysis-service@TU_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/secretmanager.secretAccessor"

# Descargar clave de service account
gcloud iam service-accounts keys create service-account-key.json \
    --iam-account=data-analysis-service@TU_PROJECT_ID.iam.gserviceaccount.com
```

### 3. Habilitar APIs Necesarias

```bash
gcloud services enable run.googleapis.com
gcloud services enable cloudscheduler.googleapis.com
gcloud services enable firestore.googleapis.com
gcloud services enable secretmanager.googleapis.com
gcloud services enable cloudbuild.googleapis.com
```

### 4. Configurar Firestore

```bash
# Crear base de datos Firestore en modo nativo
gcloud firestore databases create --region=us-central1
```

### 5. Crear Secretos (Opcional)

```bash
# Ejemplo: Crear secreto para configuraciÃ³n
echo '{"api_key": "tu-api-key"}' | \
gcloud secrets create workflow-config \
    --data-file=- \
    --replication-policy="automatic"
```

---

## ğŸ’» EjecuciÃ³n Local

### 1. Activar Entorno Virtual

```bash
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows
```

### 2. Configurar Variables de Entorno

```bash
# Cargar variables desde .env
export $(cat .env | xargs)

# O manualmente:
export GCP_PROJECT_ID=tu-proyecto-id
export GOOGLE_APPLICATION_CREDENTIALS=/ruta/a/service-account-key.json
```

### 3. Ejecutar el Servidor

```bash
# Modo desarrollo
python main.py

# O usando uvicorn directamente
uvicorn main:app --reload --host 0.0.0.0 --port 8080
```

### 4. Verificar que Funciona

Abre tu navegador en `http://localhost:8080` o usa curl:

```bash
# Health check
curl http://localhost:8080/health

# Ver informaciÃ³n del servicio
curl http://localhost:8080/

# Ver estado del workflow
curl http://localhost:8080/status
```

### 5. Ejecutar el Workflow Manualmente

```bash
# EjecuciÃ³n sÃ­ncrona
curl -X POST http://localhost:8080/run-analysis

# EjecuciÃ³n asÃ­ncrona (background)
curl -X POST "http://localhost:8080/run-analysis?async_execution=true"

# Con retry
curl -X POST "http://localhost:8080/run-analysis?retry=true"
```

---

## ğŸŒ Despliegue a Cloud Run

### OpciÃ³n A: Usando el Script de Despliegue

```bash
# Configurar variables de entorno
export GCP_PROJECT_ID=tu-proyecto-id
export GCP_REGION=us-central1

# Ejecutar despliegue
./scripts/deploy.sh production
```

### OpciÃ³n B: Despliegue Manual

```bash
# 1. Build de la imagen
gcloud builds submit --tag gcr.io/TU_PROJECT_ID/data-analysis-service

# 2. Deploy a Cloud Run
gcloud run deploy data-analysis-service \
    --image gcr.io/TU_PROJECT_ID/data-analysis-service \
    --platform managed \
    --region us-central1 \
    --allow-unauthenticated \
    --memory 512Mi \
    --cpu 1 \
    --timeout 3600 \
    --max-instances 10 \
    --set-env-vars "ENVIRONMENT=production,GCP_PROJECT_ID=TU_PROJECT_ID,WORKFLOW_NAME=daily_data_analysis"

# 3. Obtener URL del servicio
gcloud run services describe data-analysis-service \
    --platform managed \
    --region us-central1 \
    --format 'value(status.url)'
```

### Configurar Variables de Entorno en Cloud Run

```bash
gcloud run services update data-analysis-service \
    --region us-central1 \
    --set-env-vars "GCP_PROJECT_ID=TU_PROJECT_ID,WORKFLOW_NAME=daily_data_analysis"
```

### Configurar Secretos en Cloud Run

```bash
gcloud run services update data-analysis-service \
    --region us-central1 \
    --update-secrets CONFIG_SECRET_NAME=workflow-config:latest
```

---

## â° ConfiguraciÃ³n del Cron Job

### OpciÃ³n A: Usando el Script

```bash
export GCP_PROJECT_ID=tu-proyecto-id
export GCP_REGION=us-central1

./scripts/setup-scheduler.sh
```

### OpciÃ³n B: Manual

```bash
# Obtener URL del servicio
SERVICE_URL=$(gcloud run services describe data-analysis-service \
    --platform managed \
    --region us-central1 \
    --format 'value(status.url)')

# Crear Cloud Scheduler job
gcloud scheduler jobs create http daily-data-analysis-job \
    --location=us-central1 \
    --schedule="0 3 * * *" \
    --uri="${SERVICE_URL}/run-analysis" \
    --http-method=POST \
    --oidc-service-account-email=data-analysis-service@TU_PROJECT_ID.iam.gserviceaccount.com \
    --time-zone="America/New_York"
```

### Probar el Cron Job Manualmente

```bash
gcloud scheduler jobs run daily-data-analysis-job \
    --location=us-central1
```

### Modificar el Schedule

```bash
# Formato cron: "minuto hora dÃ­a mes dÃ­a_semana"
# Ejemplos:
# - "0 3 * * *"      -> Diario a las 3 AM
# - "0 */6 * * *"    -> Cada 6 horas
# - "0 9 * * 1"      -> Lunes a las 9 AM
# - "30 2 * * 1-5"   -> Lunes-Viernes a las 2:30 AM

gcloud scheduler jobs update http daily-data-analysis-job \
    --location=us-central1 \
    --schedule="0 2 * * *"  # Cambiar a 2 AM
```

---

## ğŸ§ª Pruebas

### Testing Local

```bash
# Usar el script de testing
./scripts/test-service.sh http://localhost:8080
```

### Testing en Cloud Run

```bash
# Obtener URL del servicio
SERVICE_URL=$(gcloud run services describe data-analysis-service \
    --platform managed \
    --region us-central1 \
    --format 'value(status.url)')

# Ejecutar tests
./scripts/test-service.sh $SERVICE_URL
```

### Tests Manuales de Endpoints

```bash
# 1. Health Check
curl $SERVICE_URL/health

# 2. Estado del workflow
curl $SERVICE_URL/status

# 3. Ejecutar anÃ¡lisis (async)
curl -X POST "$SERVICE_URL/run-analysis?async_execution=true"

# 4. Ver historial de ejecuciones
curl "$SERVICE_URL/history?limit=5"

# 5. Resetear workflow
curl -X POST $SERVICE_URL/reset
```

---

## ğŸ—ï¸ Arquitectura

### Componentes Principales

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cloud Scheduler    â”‚  â† Trigger diario (3 AM)
â”‚    (Cron Job)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ POST /run-analysis
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Cloud Run        â”‚
â”‚  (FastAPI Service)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Saga Orchestrator  â”‚  â† Maneja el flujo y compensaciones
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Workflow Steps (en orden):             â”‚
â”‚  1. ExtractStep    â†’ Extrae datos       â”‚
â”‚  2. TransformStep  â†’ Limpia/normaliza   â”‚
â”‚  3. AnalyzeStep    â†’ Calcula mÃ©tricas   â”‚
â”‚  4. StoreStep      â†’ Guarda resultados  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Firestore       â”‚  â† Estado del workflow
â”‚  - workflow_runs    â”‚
â”‚  - history          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de EjecuciÃ³n

1. **Cloud Scheduler** dispara el workflow enviando POST a `/run-analysis`
2. **SagaOrchestrator** lee el estado desde Firestore
3. Ejecuta cada step en orden:
   - Si un step falla, ejecuta compensaciones en orden inverso
   - Guarda estado despuÃ©s de cada step exitoso
4. Al completar, guarda resultados y actualiza historial

### PatrÃ³n Saga

Cada step implementa:
- `run()`: LÃ³gica principal del paso
- `compensate()`: LÃ³gica de rollback/compensaciÃ³n

Si el paso 3 falla, se ejecutan compensaciones de los pasos 2 y 1.

---

## ğŸ” SoluciÃ³n de Problemas

### Error: "Module not found"

```bash
# AsegÃºrate de que el entorno virtual estÃ© activado
source venv/bin/activate

# Reinstala dependencias
pip install -r requirements.txt
```

### Error: "Permission denied" en Firestore

```bash
# Verifica roles del service account
gcloud projects get-iam-policy TU_PROJECT_ID \
    --flatten="bindings[].members" \
    --filter="bindings.members:serviceAccount:data-analysis-service@*"

# Asignar rol necesario
gcloud projects add-iam-policy-binding TU_PROJECT_ID \
    --member="serviceAccount:data-analysis-service@TU_PROJECT_ID.iam.gserviceaccount.com" \
    --role="roles/datastore.user"
```

### Error: "Secret not found"

```bash
# Listar secretos disponibles
gcloud secrets list

# Verificar permisos
gcloud secrets get-iam-policy workflow-config
```

### El workflow no se ejecuta desde Cloud Scheduler

```bash
# Ver logs del scheduler
gcloud scheduler jobs describe daily-data-analysis-job \
    --location=us-central1

# Ver logs de Cloud Run
gcloud logs read --service=data-analysis-service \
    --limit=50 \
    --format=json
```

### Debugging Local

```bash
# Activar modo debug en main.py
export LOG_LEVEL=DEBUG

# Ver logs detallados
python main.py
```

### Ver Estado del Workflow

```bash
# Consultar Firestore directamente
gcloud firestore databases export gs://TU_BUCKET/backup \
    --collection-ids=workflow_runs

# O usar la API
curl http://localhost:8080/status | jq '.'
```

### Resetear Workflow Bloqueado

```bash
# Via API
curl -X POST http://localhost:8080/reset

# O manualmente en Firestore Console
# Ir a: Firestore > workflow_runs > daily_data_analysis
# Editar status a "NOT_STARTED"
```

---

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n

- [DocumentaciÃ³n de FastAPI](https://fastapi.tiangolo.com/)
- [Cloud Run Docs](https://cloud.google.com/run/docs)
- [Cloud Scheduler Docs](https://cloud.google.com/scheduler/docs)
- [Firestore Docs](https://cloud.google.com/firestore/docs)

### Monitoreo

```bash
# Ver logs en tiempo real
gcloud logs tail --service=data-analysis-service

# MÃ©tricas de Cloud Run
gcloud monitoring dashboards list
```

### Estructura del Proyecto

```
AnalistsDIARIO/
â”œâ”€â”€ main.py                 # Entry point FastAPI
â”œâ”€â”€ saga_orchestrator.py    # Orquestador Saga
â”œâ”€â”€ firestore_repo.py       # Repositorio Firestore
â”œâ”€â”€ secrets_manager.py      # Manejo de secretos
â”œâ”€â”€ requirements.txt        # Dependencias Python
â”œâ”€â”€ Dockerfile             # ContainerizaciÃ³n
â”œâ”€â”€ .env.example           # Template de variables
â”œâ”€â”€ steps/                 # Pasos del workflow
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py         # ExtracciÃ³n de datos
â”‚   â”œâ”€â”€ transform.py       # TransformaciÃ³n
â”‚   â”œâ”€â”€ analyze.py         # AnÃ¡lisis
â”‚   â””â”€â”€ store.py           # Almacenamiento
â””â”€â”€ scripts/               # Scripts de deployment
    â”œâ”€â”€ deploy.sh
    â”œâ”€â”€ setup-scheduler.sh
    â”œâ”€â”€ local-setup.sh
    â””â”€â”€ test-service.sh
```

---

## ğŸ¤ Contribuciones

Para contribuir al proyecto:

1. Fork el repositorio
2. Crea una rama: `git checkout -b feature/nueva-funcionalidad`
3. Commit cambios: `git commit -am 'Agregar nueva funcionalidad'`
4. Push: `git push origin feature/nueva-funcionalidad`
5. Crear Pull Request

---

## ğŸ“„ Licencia

[Especifica tu licencia aquÃ­]

---

## ğŸ’¬ Soporte

Para preguntas o problemas:
- Crear un issue en GitHub
- Contactar al equipo de desarrollo

---

**Â¡Listo para comenzar! ğŸš€**
